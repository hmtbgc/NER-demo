{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"./data\"\n",
    "train_data_path = os.path.join(root, \"train_data.txt\")\n",
    "test_data_path = os.path.join(root, \"test_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.7\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(path):\n",
    "    out = []\n",
    "    with open(path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "        sentence = []\n",
    "        labels = []\n",
    "        for line in tqdm(f):\n",
    "            s = line.strip().split(\"\\t\")\n",
    "            if (len(s) == 2):\n",
    "                word, label = s[0], s[1]\n",
    "            else:\n",
    "                word, label = \"[PAD]\", s[0]\n",
    "            sentence.append(word)\n",
    "            labels.append(label)\n",
    "            if (word == 'ã€‚'):\n",
    "                out.append((sentence, labels))\n",
    "                sentence = []\n",
    "                labels = []\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "418362it [00:00, 2175048.04it/s]\n",
      "132709it [00:00, 2225446.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total sentences in train data: 5425\n",
      "total sentences in valid data: 2325\n",
      "total sentences in test data: 2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = get_sentences(train_data_path)\n",
    "test_data = get_sentences(test_data_path)\n",
    "\n",
    "np.random.shuffle(train_data)\n",
    "train_number = int(train_ratio * len(train_data))\n",
    "valid_data = train_data[train_number : ]\n",
    "train_data = train_data[ : train_number]\n",
    "print(f\"total sentences in train data: {len(train_data)}\")\n",
    "print(f\"total sentences in valid data: {len(valid_data)}\")\n",
    "print(f\"total sentences in test data: {len(test_data)}\")\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tot_labels = ['[PAD]', '[CLS]', '[SEP]', 'O', 'B-BODY','I-TEST', 'I-EXAMINATIONS',\n",
    "            'I-TREATMENT', 'B-DRUG', 'B-TREATMENT', 'I-DISEASES', 'B-EXAMINATIONS',\n",
    "            'I-BODY', 'B-TEST', 'B-DISEASES', 'I-DRUG']\n",
    "label2idx = {label : idx for idx, label in enumerate(tot_labels)}\n",
    "idx2label = {idx : label for idx, label in enumerate(tot_labels)}\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, MAX_LEN=256-2):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sentences = []\n",
    "        self.labels = []\n",
    "        for sentence, labels in self.data:\n",
    "            if (len(sentence) > MAX_LEN):\n",
    "                sentence = sentence[ : MAX_LEN]\n",
    "                labels = labels[ : MAX_LEN]\n",
    "            sentence = [\"[CLS]\"] + sentence + [\"[SEP]\"]\n",
    "            labels = [\"[CLS]\"] + labels + [\"[SEP]\"]\n",
    "            self.sentences.append(tokenizer.convert_tokens_to_ids(sentence))\n",
    "            self.labels.append([label2idx[label] for label in labels])\n",
    "             \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.sentences[index], self.labels[index]\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        max_len = max([len(data[0]) for data in batch])\n",
    "        PAD_ID = self.tokenizer.pad_token_id\n",
    "        sentence_tensors = torch.LongTensor([data[0] + [PAD_ID] * (max_len - len(data[0])) for data in batch])\n",
    "        labels_tensors = torch.LongTensor([data[1] + [label2idx[\"[PAD]\"]] * (max_len - len(data[1])) for data in batch])\n",
    "        masks = (sentence_tensors != PAD_ID)\n",
    "        return sentence_tensors, labels_tensors, masks\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "train_dataset = NERDataset(train_data, tokenizer)\n",
    "valid_dataset = NERDataset(valid_data, tokenizer)\n",
    "test_dataset = NERDataset(test_data, tokenizer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101,  123,  121,  ...,    0,    0,    0],\n",
      "        [ 101, 8020, 7360,  ...,    0,    0,    0],\n",
      "        [ 101, 4680, 1184,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 8024, 5357,  ...,    0,    0,    0],\n",
      "        [ 101,  754,  123,  ...,    0,    0,    0],\n",
      "        [ 101, 2642, 5442,  ...,    0,    0,    0]])\n",
      "tensor([[1, 3, 3,  ..., 0, 0, 0],\n",
      "        [1, 3, 4,  ..., 0, 0, 0],\n",
      "        [1, 3, 3,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 3, 3,  ..., 0, 0, 0],\n",
      "        [1, 3, 3,  ..., 0, 0, 0],\n",
      "        [1, 3, 3,  ..., 0, 0, 0]])\n",
      "tensor([[ True,  True,  True,  ..., False, False, False],\n",
      "        [ True,  True,  True,  ..., False, False, False],\n",
      "        [ True,  True,  True,  ..., False, False, False],\n",
      "        ...,\n",
      "        [ True,  True,  True,  ..., False, False, False],\n",
      "        [ True,  True,  True,  ..., False, False, False],\n",
      "        [ True,  True,  True,  ..., False, False, False]])\n",
      "torch.Size([128, 256])\n",
      "torch.Size([128, 256])\n",
      "torch.Size([128, 256])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_dataset.collate_fn, num_workers=4)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=valid_dataset.collate_fn, num_workers=4)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=test_dataset.collate_fn, num_workers=4)\n",
    "\n",
    "for data in train_dataloader:\n",
    "    print(data[0])\n",
    "    print(data[1])\n",
    "    print(data[2])\n",
    "    print(data[0].shape)\n",
    "    print(data[1].shape)\n",
    "    print(data[2].shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "from torchcrf import CRF\n",
    "import torch.nn as nn\n",
    "\n",
    "class BertCRF(nn.Module):\n",
    "    def __init__(self, target_size, hidden_dim=768):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-chinese\")\n",
    "        self.crf = CRF(target_size, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, target_size)\n",
    "        \n",
    "    def forward(self, sentences, labels, mask):\n",
    "        with torch.no_grad():\n",
    "            output = self.bert(input_ids=sentences, attention_mask=mask)\n",
    "        last_hidden_states = output.last_hidden_state\n",
    "        out = self.linear(last_hidden_states)\n",
    "        loss = -self.crf(out, labels, mask, reduction=\"mean\")\n",
    "        return loss\n",
    "    \n",
    "    def decode(self, sentences, mask):\n",
    "        with torch.no_grad():\n",
    "            output = self.bert(input_ids=sentences, attention_mask=mask)\n",
    "            last_hidden_states = output.last_hidden_state\n",
    "            out = self.linear(last_hidden_states)\n",
    "            decode = self.crf.decode(out, mask)\n",
    "            return decode\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "target_size = len(label2idx)\n",
    "model = BertCRF(target_size=target_size)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=0.001, weight_decay=1e-5, eps=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger('my_logger')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "file_handler = logging.FileHandler('train.log')\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(file_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    predicted = []\n",
    "    truth = []\n",
    "    for data in tqdm(dataloader):\n",
    "        sentences, labels, masks = data\n",
    "        sentences = sentences.to(device)\n",
    "        labels = labels.to(device)\n",
    "        masks = masks.to(device)\n",
    "        out = model.decode(sentences, masks)\n",
    "        for temp in out:\n",
    "            predicted.extend(temp)\n",
    "        y_origin = torch.masked_select(labels, masks)\n",
    "        truth.extend(y_origin.cpu().tolist())\n",
    "    predicted = np.array(predicted)\n",
    "    truth = np.array(truth)\n",
    "    return predicted, truth, (predicted == truth).mean()\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:55<00:00,  1.28s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:28<00:00,  1.50s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:54<00:00,  1.27s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:28<00:00,  1.50s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:54<00:00,  1.27s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:28<00:00,  1.50s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:55<00:00,  1.29s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:28<00:00,  1.49s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:57<00:00,  1.33s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:28<00:00,  1.51s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:54<00:00,  1.28s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:27<00:00,  1.47s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:53<00:00,  1.25s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:28<00:00,  1.48s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:56<00:00,  1.32s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:28<00:00,  1.48s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:55<00:00,  1.30s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:27<00:00,  1.47s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:56<00:00,  1.30s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:28<00:00,  1.48s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:56<00:00,  1.31s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:28<00:00,  1.48s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:55<00:00,  1.28s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:28<00:00,  1.49s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:55<00:00,  1.30s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:27<00:00,  1.46s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:56<00:00,  1.31s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:28<00:00,  1.48s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:56<00:00,  1.31s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:27<00:00,  1.47s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:55<00:00,  1.29s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:28<00:00,  1.50s/it]\n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 23/43 [00:31<00:27,  1.35s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m masks \u001b[39m=\u001b[39m masks\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 15\u001b[0m loss \u001b[39m=\u001b[39m model(sentences, labels, masks)\n\u001b[1;32m     16\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     17\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt2/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[81], line 18\u001b[0m, in \u001b[0;36mBertCRF.forward\u001b[0;34m(self, sentences, labels, mask)\u001b[0m\n\u001b[1;32m     16\u001b[0m last_hidden_states \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mlast_hidden_state\n\u001b[1;32m     17\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear(last_hidden_states)\n\u001b[0;32m---> 18\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcrf(out, labels, mask, reduction\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmean\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     19\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt2/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt2/lib/python3.10/site-packages/torchcrf/__init__.py:90\u001b[0m, in \u001b[0;36mCRF.forward\u001b[0;34m(self, emissions, tags, mask, reduction)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m     64\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m     65\u001b[0m         emissions: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m         reduction: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msum\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     69\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m     70\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute the conditional log likelihood of a sequence of tags given emission scores.\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \n\u001b[1;32m     72\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39m        reduction is ``none``, ``()`` otherwise.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate(emissions, tags\u001b[39m=\u001b[39;49mtags, mask\u001b[39m=\u001b[39;49mmask)\n\u001b[1;32m     91\u001b[0m     \u001b[39mif\u001b[39;00m reduction \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msum\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtoken_mean\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     92\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39minvalid reduction: \u001b[39m\u001b[39m{\u001b[39;00mreduction\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt2/lib/python3.10/site-packages/torchcrf/__init__.py:166\u001b[0m, in \u001b[0;36mCRF._validate\u001b[0;34m(self, emissions, tags, mask)\u001b[0m\n\u001b[1;32m    164\u001b[0m no_empty_seq \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m mask[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mall()\n\u001b[1;32m    165\u001b[0m no_empty_seq_bf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m mask[:, \u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mall()\n\u001b[0;32m--> 166\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_empty_seq \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_empty_seq_bf:\n\u001b[1;32m    167\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mmask of the first timestep must all be on\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Epoch = 30\n",
    "eval_every = 1\n",
    "early_stop = 5\n",
    "\n",
    "best_acc = 0\n",
    "best_epoch = -1\n",
    "best_model_pt_path = \"model.pt\"\n",
    "\n",
    "for epoch in range(Epoch):\n",
    "    epoch_loss = 0\n",
    "    for data in tqdm(train_dataloader):\n",
    "        sentences, labels, masks = data\n",
    "        sentences = sentences.to(device)\n",
    "        labels = labels.to(device)\n",
    "        masks = masks.to(device)\n",
    "        loss = model(sentences, labels, masks)\n",
    "        epoch_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    logger.info(f\"Epoch:{epoch}, loss:{epoch_loss:.4f}\")\n",
    "    if (epoch % eval_every == 0):\n",
    "        acc = evaluate(model, valid_dataloader)[-1]\n",
    "        if (acc > best_acc):\n",
    "            best_acc = acc\n",
    "            best_epoch = epoch\n",
    "            torch.save(model.state_dict(), best_model_pt_path)\n",
    "        else:\n",
    "            early_stop -= 1\n",
    "        logger.info(f\"Epoch:{epoch}, valid acc:{acc * 100:.2f}%, current best valid acc:{best_acc * 100:.2f}%, at epoch {best_epoch}\")\n",
    "        if (early_stop == 0):\n",
    "            logger.info(f\"Early stop!\")\n",
    "            break\n",
    "        \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:30<00:00,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 93.82%\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "        B-BODY       0.83      0.84      0.83      3078\n",
      "        I-TEST       0.49      0.33      0.40      1604\n",
      "I-EXAMINATIONS       0.85      0.76      0.80       833\n",
      "   I-TREATMENT       0.90      0.84      0.87      1870\n",
      "        B-DRUG       0.74      0.62      0.67       480\n",
      "   B-TREATMENT       0.78      0.61      0.69       162\n",
      "    I-DISEASES       0.83      0.76      0.79      6843\n",
      "B-EXAMINATIONS       0.80      0.66      0.72       345\n",
      "        I-BODY       0.69      0.75      0.72      2434\n",
      "        B-TEST       0.46      0.27      0.34       567\n",
      "    B-DISEASES       0.75      0.67      0.71      1310\n",
      "        I-DRUG       0.85      0.73      0.79      1432\n",
      "\n",
      "     micro avg       0.79      0.72      0.75     20958\n",
      "     macro avg       0.75      0.65      0.69     20958\n",
      "  weighted avg       0.78      0.72      0.74     20958\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "test_model = BertCRF(target_size=target_size)\n",
    "test_model.load_state_dict(torch.load(\"model.pt\"))\n",
    "test_model = test_model.to(device)\n",
    "predicted, truth, test_acc = evaluate(test_model, test_dataloader)\n",
    "print(f\"test accuracy: {test_acc * 100:.2f}%\")\n",
    "predicted = [idx2label[idx] for idx in predicted]\n",
    "truth = [idx2label[idx] for idx in truth]\n",
    "selected_labels = ['B-BODY','I-TEST', 'I-EXAMINATIONS',\n",
    "            'I-TREATMENT', 'B-DRUG', 'B-TREATMENT', 'I-DISEASES', 'B-EXAMINATIONS',\n",
    "            'I-BODY', 'B-TEST', 'B-DISEASES', 'I-DRUG']\n",
    "report = classification_report(truth, predicted, labels=selected_labels)\n",
    "print(report)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
